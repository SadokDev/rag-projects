{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets torch faiss-cpu wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets torch faiss-cpu wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot(data):\n",
    "    # Apply t-SNE to reduce to 3D\n",
    "    tsne = TSNE(n_components=3, random_state=42,perplexity=data.shape[0]-1)\n",
    "    data_3d = tsne.fit_transform(data)\n",
    "    \n",
    "    # Plotting\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Assign colors for each point based on its index\n",
    "    num_points = len(data_3d)\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, num_points))\n",
    "    \n",
    "    # Plot scatter with unique colors for each point\n",
    "    for idx, point in enumerate(data_3d):\n",
    "        ax.scatter(point[0], point[1], point[2], label=str(idx), color=colors[idx])\n",
    "    \n",
    "    # Adding labels and titles\n",
    "    ax.set_xlabel('TSNE Component 1')\n",
    "    ax.set_ylabel('TSNE Component 2')\n",
    "    ax.set_zlabel('TSNE Component 3')\n",
    "    plt.title('3D t-SNE Visualization')\n",
    "    plt.legend(title='Input Order')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the notebook focus on preparing the data for question answering system. You can download a specific file or use one of your own, then read it and process it to make it suitable for NLP tasks. I will use a company policies text file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading and preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_split_text(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    # Split the text into paragraphs (simple split by newline characters)\n",
    "    paragraphs = text.split('\\n')\n",
    "    # Filter out any empty paragraphs or undesired entries\n",
    "    paragraphs = [para.strip() for para in paragraphs if len(para.strip()) > 0]\n",
    "    return paragraphs\n",
    "\n",
    "# Read the text file and split it into paragraphs\n",
    "paragraphs = read_and_split_text('companyPolicies.txt')\n",
    "paragraphs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paragraph samples\n",
    "for i in range(4):\n",
    "    print(f\"sample: {i} paragraph: {paragraphs[i]} \\n\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever : Encoding and Indexing\n",
    "In this section, I will prepare my text data for efficient retrieval by encoding the paragraphs into vector embeddings, i.e., contextual embeddings, and then indexing these embeddings using FAISS. This allows my question-answering system to quickly find the most relevant information when processing queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding texts into embeddings\n",
    "Let's use the Dense Passage Retriever (DPR) model, specifically the context encoder, to convert the preprocessed text data into dense vector embeddings. These embeddings capture the semantic meanings of the texts, enabling effective similarity-based retrieval. DPR models, such as the the DPRContextEncoder and DPRContextEncoderTokenizer, are built on the BERT architecture but specialize in dense passage retrieval. They differ from BERT in their training, which focuses on contrastive learning for retrieving relevant passages, while BERT is more general-purpose, handling various NLP tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Tokenization**: Each text is tokenized to format it in a way that is compatible with the encoder. This involves converting text into a sequence of tokens with attention masks, ensuring uniform length through padding and managing text size through truncation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "context_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "context_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [(\"How are you?\", \"I am fine.\"), (\"What's up?\", \"Not much.\")]\n",
    "tokens_info=context_tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=256)\n",
    "tokens_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you can observe :\n",
    "- `token_type_ids`: These are segment embeddings used to distinguish different sentences or segments within the input. This is particularly useful in tasks that involve multiple types of input, such as question answering, where questions and context may need to be differentiated.\n",
    "\n",
    "- `attention_mask`: The attention mask indicates which tokens should be attended to by the model. It has a value of 1 for actual tokens in the input sentences and 0 for padding tokens, ensuring that the model focuses only on meaningful data.\n",
    "\n",
    "-  `input_ids`: These represent the indices of tokens in the tokenizer's vocabulary. To translate these indices back into readable tokens, you can use the method `convert_ids_to_tokens` provided by the tokenizer. Here's an example of how to use this method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in tokens_info['input_ids']:\n",
    "   print(context_tokenizer.convert_ids_to_tokens(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Encoding**: The tokenized texts are then fed into the `context_encoder`. This model processes the inputs and produces a pooled output for each, effectively compressing the information of an entire text into a single, dense vector embedding that represents the semantic essence of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_encoder = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `context_tokenizer` and `context_encoder` work together to process text data, transforming paragraphs into contextual embeddings suitable for further NLP tasks. Here's how these components are applied to the first 20 paragraphs from a list:\n",
    "   - The `context_tokenizer` takes the first 20 paragraphs and converts each into a sequence of token IDs, formatted specifically as input to a PyTorch model. This process includes:\n",
    "     - **Padding**: To ensure uniformity, shorter text sequences are padded with zeros to reach the specified maximum length of 256 tokens.\n",
    "     - **Truncation**: Longer texts are cut off at 256 tokens to maintain consistency across all inputs.\n",
    "   - The tokenized data is then passed to the `context_encoder`, which processes these token sequences to produce contextual embeddings. Each output embedding vector from the encoder represents the semantic content of its corresponding paragraph, encapsulating key informational and contextual nuances.\n",
    "   - The encoder outputs a PyTorch tensor where each row corresponds to a different paragraph's embedding. The shape of this tensor, determined by the number of paragraphs processed and the embedding dimensions, reflects the detailed, contextualized representation of each paragraph's content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(paragraphs)\n",
    "tokens=context_tokenizer( paragraphs[:20], return_tensors='pt', padding=True, truncation=True, max_length=256) \n",
    "tokens\n",
    "outputs=context_encoder(**tokens)\n",
    "outputs.pooler_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the resulting shape\n",
    "outputs.pooler_output.shape # torch.Size([20, 768])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing high dimentional data usine t-SNE\n",
    "t-distributed stochastic neighbor embedding (t-SNE) is a statistical method for visualizing high-dimensional data by giving each datapoint a location in a two or three-dimensional map. I will use to analyze outputs from DPRContextEncoder.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot(outputs.pooler_output.detach().numpy()) # The 3D graph will show the samples that are closer to each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **3. Aggregation**: The generated embeddings from the texts are then aggregated into a single `NumPy` array. It is essential for indexing/similarity searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_contexts(text_list):\n",
    "    # Encode a list of texts into embeddings\n",
    "    embeddings = []\n",
    "    for text in text_list:\n",
    "        inputs = context_tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=256)\n",
    "        outputs = context_encoder(**inputs)\n",
    "        embeddings.append(outputs.pooler_output)\n",
    "    return torch.cat(embeddings).detach().numpy()\n",
    "\n",
    "# you would now encode these paragraphs to create embeddings.\n",
    "context_embeddings = encode_contexts(paragraphs) # context_embeddings.shape : (76, 768)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAISS Index : Create and populate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faiss (Facebook AI Similarity Search) is a library that allows developers to quickly search for embeddings of multimedia documents that are similar to each other.\n",
    "`IndexFlatL2` (one of the most used indexes in FAISS) measures the L2 (or Euclidean) distance between all given points between our query vector, and the vectors loaded into the index. It’s simple, very accurate, but not too fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "# Convert list of numpy arrays into a single numpy array\n",
    "embedding_dim = 768  # This should match the dimension of your embeddings\n",
    "context_embeddings_np = np.array(context_embeddings).astype('float32')\n",
    "\n",
    "# Create a FAISS index for the embeddings\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "index.add(context_embeddings_np)  # Add the context embeddings to the index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use the ```DPRQuestionEncoder``` and ```DPRQuestionEncoderTokenizer``` for encoding questions(to process and encode queries (questions) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question encoder transforms questions into dense embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corresponding tokenizer standardizes the questions to ensure they are correctly formatted for the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DPR question encoder and tokenizer\n",
    "question_encoder = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search the prebuilt FAISS index to find the most relevant contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the official documentation for the use of the FAISS index in retrieving information based on query similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_relevant_contexts(question, question_tokenizer, question_encoder, index, k=5):\n",
    "    \"\"\"\n",
    "    Searches for the most relevant contexts to a given question.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Distances and indices of the top k relevant contexts.\n",
    "    \"\"\"\n",
    "    # Tokenize the question\n",
    "    question_inputs = question_tokenizer(question, return_tensors='pt')\n",
    "\n",
    "    # Encode the question to get the embedding\n",
    "    question_embedding = question_encoder(**question_inputs).pooler_output.detach().numpy()\n",
    "\n",
    "    # Search the index to retrieve top k relevant contexts\n",
    "    # D is an array containing the distances between the query embedding and the retrieved document embeddings\n",
    "    # I contains the indices of the paragraphs (paragraphs array) that have been identified as the most relevant to the query\n",
    "    D, I = index.search(question_embedding, k)\n",
    "\n",
    "    return D, I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving response generation with LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LLM possesses a general knowledge, it has no idea about my specific dataset. I will use the GPT2 model (GPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will combine the use of GPT2 for generation and DPR for question encoding to create a strong framework for my natural language processing application, enabling it to deliver accurate and context-aware responses to user inquiries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate answer without adding context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_without_context(question):\n",
    "    # Tokenize the input question\n",
    "    inputs = tokenizer(question, return_tensors='pt', max_length=1024, truncation=True)\n",
    "    \n",
    "    # Generate output directly from the question without additional context\n",
    "    summary_ids = model.generate(inputs['input_ids'], max_length=150, min_length=40, length_penalty=2.0,\n",
    "                                 num_beams=4, early_stopping=True,pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    # Decode and return the generated text\n",
    "    answer = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate answers using DPR contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question, contexts):\n",
    "    # Concatenate the retrieved contexts to form the input to GPT2\n",
    "    input_text = question + ' ' + ' '.join(contexts)\n",
    "    inputs = tokenizer(input_text, return_tensors='pt', max_length=1024, truncation=True)\n",
    "\n",
    "    # Generate output using GPT2\n",
    "    summary_ids = model.generate(inputs['input_ids'], max_new_tokens=50, min_length=40, length_penalty=2.0,\n",
    "                                 num_beams=4, early_stopping=True,pad_token_id=tokenizer.eos_token_id)\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is smoking policy?\"\n",
    "\n",
    "_,I =search_relevant_contexts(question, question_tokenizer, question_encoder, index, k=5)\n",
    "\n",
    "print(f\"paragraphs indexes {I}\")\n",
    "\n",
    "top_cont = [paragraphs[idx] for idx in I[0]] \n",
    "print(f\"top_contexts {top_cont}\")\n",
    "\n",
    "answer = generate_answer(question, top_cont)\n",
    "print(\"Generated Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrating DPR retrieval with generative models such as GPT2, leads to more effective and contextually relevant answers. Combining retrieval and generation techniques in natural language processing applications, where the context is provided by DPR, can strongly improve the quality of the generated content.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
